{"_id":"csv","_rev":"96-5b8e8b1fb863ecd2b757249c60b7e27c","name":"csv","description":"CSV parser with simple api, full of options and tested against large datasets.","dist-tags":{"stable":"0.0.2","latest":"0.3.3"},"versions":{"0.0.1":{"name":"csv","version":"0.0.1","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"_id":"csv@0.0.1","_nodeSupported":true,"_npmVersion":"0.2.7-2","_nodeVersion":"v0.3.1-pre","dist":{"tarball":"http://registry.npmjs.org/csv/-/csv-0.0.1.tgz","shasum":"69744756a53c7aeb299797f337530f151fbb2e0d"}},"0.0.2":{"name":"csv","version":"0.0.2","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"_id":"csv@0.0.2","_nodeSupported":true,"_npmVersion":"0.2.7-2","_nodeVersion":"v0.3.1-pre","dist":{"tarball":"http://registry.npmjs.org/csv/-/csv-0.0.2.tgz","shasum":"db442d9a2a1de418874595dec61f5bc5301497e6"}},"0.0.3":{"name":"csv","version":"0.0.3","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"repository":{"type":"git","url":"https://github.com/wdavidw/node-csv-parser.git"},"_id":"csv@0.0.3","_nodeSupported":true,"_npmVersion":"0.2.3-6","_nodeVersion":"v0.3.2-pre","dist":{"tarball":"http://registry.npmjs.org/csv/-/csv-0.0.3.tgz","shasum":"769581ead7df296bbdddaaa59247ab5470d6f70f"}},"0.0.5":{"name":"csv","version":"0.0.5","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"https://github.com/wdavidw/node-csv-parser.git"},"_id":"csv@0.0.5","_nodeSupported":true,"_npmVersion":"0.2.3-6","_nodeVersion":"v0.3.3-pre","dist":{"tarball":"http://registry.npmjs.org/csv/-/csv-0.0.5.tgz","shasum":"6ba84ea2930dccbd1a4e3faf473cc19f60523454"}},"0.0.6":{"name":"csv","version":"0.0.6","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"https://github.com/wdavidw/node-csv-parser.git"},"_id":"csv@0.0.6","_nodeSupported":true,"_npmVersion":"0.2.3-6","_nodeVersion":"v0.3.6-pre","dist":{"tarball":"http://registry.npmjs.org/csv/-/csv-0.0.6.tgz","shasum":"4755b111df196de303e14c9461280c91bad3d70c"}},"0.0.7":{"name":"csv","version":"0.0.7","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"https://github.com/wdavidw/node-csv-parser.git"},"_id":"csv@0.0.7","_engineSupported":true,"_npmVersion":"0.2.18","_nodeVersion":"v0.4.0","files":[""],"_defaultsLoaded":true,"dist":{"shasum":"65190b9c40c35f07636987bbef6dc87e01bb4b13","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.7.tgz"}},"0.0.9":{"name":"csv","version":"0.0.9","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"_id":"csv@0.0.9","_engineSupported":true,"_npmVersion":"0.3.15","_nodeVersion":"v0.5.0-pre","files":[""],"_defaultsLoaded":true,"dist":{"shasum":"09fb6254c7a9341e9aee101d64375285ab0ea8ac","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.9.tgz"}},"0.0.10":{"name":"csv","version":"0.0.10","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"directories":{"lib":"./lib"},"main":"./lib/csv","engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"_npmJsonOpts":{"file":"/Users/wdavidw/.npm/csv/0.0.10/package/package.json","contributors":false,"wscript":false,"serverjs":false},"_id":"csv@0.0.10","dependencies":{},"devDependencies":{},"_engineSupported":true,"_npmVersion":"1.0.26","_nodeVersion":"v0.4.11","_defaultsLoaded":true,"dist":{"shasum":"7dbe276f607c1a0f412a29de977819b2ecfd8eab","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.10.tgz"},"scripts":{},"maintainers":[{"name":"david","email":"david@adaltas.com"}]},"0.0.11":{"name":"csv","version":"0.0.11","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"_npmUser":{"name":"david","email":"david@adaltas.com"},"_id":"csv@0.0.11","dependencies":{},"devDependencies":{},"_engineSupported":true,"_npmVersion":"1.1.0-beta-10","_nodeVersion":"v0.6.7","_defaultsLoaded":true,"dist":{"shasum":"8c25e88d669f5a8f43f4fc624e74a5191e1abce6","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.11.tgz"},"readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provide CSV parsing and has been tested and used on large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Asynch and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *fromStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n    \n-   *from*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, default to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns section below.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, default to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), default to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), default to false.\n\nWriting API\n-----------\n\nThe following method are available:\n\n-   *write*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *toStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Default to the delimiter read option.\n    \n-   *quote*    \n    Default to the quote read option.\n    \n-   *escape*    \n    Default to the escape read option.\n    \n-   *columns*    \n    List of fields, apply when `transform` return an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Default to 'utf8', apply when a writable stream is created.\n    \n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; default to 'auto' (discovered in source).\n    \n-   *flags*    \n    Default to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Apply when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flush into a stream. Apply when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\nTransforming data\n-----------------\n\nYou may provide a callback to the `transform` method. The contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merge in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provide a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be call if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.12":{"name":"csv","version":"0.0.12","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","_npmUser":{"name":"david","email":"david@adaltas.com"},"_id":"csv@0.0.12","_engineSupported":true,"_npmVersion":"1.1.9","_nodeVersion":"v0.6.13","_defaultsLoaded":true,"dist":{"shasum":"900d6fab35d1c5aa50747d042439f2de511dc5c6","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.12.tgz"},"readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provide CSV parsing and has been tested and used on large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Asynch and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *fromStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n    \n-   *from*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, default to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns section below.\n\t\n-   *encoding*    \n    Default to 'utf8', apply when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, default to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), default to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), default to false.\n\nWriting API\n-----------\n\nThe following method are available:\n\n-   *write*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *toStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Default to the delimiter read option.\n    \n-   *quote*    \n    Default to the quote read option.\n    \n-   *escape*    \n    Default to the escape read option.\n    \n-   *columns*    \n    List of fields, apply when `transform` return an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Default to 'utf8', apply when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; default to 'auto' (discovered in source).\n    \n-   *flags*    \n    Default to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Apply when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flush into a stream. Apply when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\nTransforming data\n-----------------\n\nYou may provide a callback to the `transform` method. The contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merge in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provide a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be call if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.13":{"name":"csv","version":"0.0.13","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","_npmUser":{"name":"david","email":"david@adaltas.com"},"_id":"csv@0.0.13","_engineSupported":true,"_npmVersion":"1.1.9","_nodeVersion":"v0.6.13","_defaultsLoaded":true,"dist":{"shasum":"78189782eb5ae7dd4f428b0a016029934234c4bc","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.13.tgz"},"readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provide CSV parsing and has been tested and used on large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Asynch and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *fromStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n    \n-   *from*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, default to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns section below.\n\t\n-   *encoding*    \n    Default to 'utf8', apply when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, default to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), default to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), default to false.\n\nWriting API\n-----------\n\nThe following method are available:\n\n-   *write*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *toStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Default to the delimiter read option.\n    \n-   *quote*    \n    Default to the quote read option.\n    \n-   *escape*    \n    Default to the escape read option.\n    \n-   *columns*    \n    List of fields, apply when `transform` return an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Default to 'utf8', apply when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; default to 'auto' (discovered in source).\n    \n-   *flags*    \n    Default to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Apply when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flush into a stream. Apply when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\nTransforming data\n-----------------\n\nYou may provide a callback to the `transform` method. The contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merge in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provide a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be call if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.14":{"name":"csv","version":"0.0.14","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","_npmUser":{"name":"david","email":"david@adaltas.com"},"_id":"csv@0.0.14","_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"b770e4e9ef18d19c728c4047557eb5748a3d3b12","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.14.tgz"},"readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provide CSV parsing and has been tested and used on large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Asynch and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *fromStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n    \n-   *from*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, default to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns section below.\n\t\n-   *encoding*    \n    Default to 'utf8', apply when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, default to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), default to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), default to false.\n\nWriting API\n-----------\n\nThe following method are available:\n\n-   *write*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *toStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Default to the delimiter read option.\n    \n-   *quote*    \n    Default to the quote read option.\n    \n-   *escape*    \n    Default to the escape read option.\n    \n-   *columns*    \n    List of fields, apply when `transform` return an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Default to 'utf8', apply when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; default to 'auto' (discovered in source).\n    \n-   *flags*    \n    Default to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Apply when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flush into a stream. Apply when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\n\nTransforming data\n-----------------\n\nYou may provide a callback to the `transform` method. The contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merge in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provide a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be call if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.15":{"name":"csv","version":"0.0.15","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","_npmUser":{"name":"david","email":"david@adaltas.com"},"_id":"csv@0.0.15","_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"a8d0f7f6c3e593044476beca0ddee3ef885f84e9","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.15.tgz"},"readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provide CSV parsing and has been tested and used on large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Asynch and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *fromStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n    \n-   *from*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, default to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns section below.\n\t\n-   *encoding*    \n    Default to 'utf8', apply when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, default to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), default to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), default to false.\n\nWriting API\n-----------\n\nThe following method are available:\n\n-   *write*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *toStream*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Default to the delimiter read option.\n    \n-   *quote*    \n    Default to the quote read option.\n    \n-   *escape*    \n    Default to the escape read option.\n    \n-   *columns*    \n    List of fields, apply when `transform` return an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Default to 'utf8', apply when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; default to 'auto' (discovered in source).\n    \n-   *flags*    \n    Default to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Apply when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flush into a stream. Apply when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\n\nTransforming data\n-----------------\n\nYou may provide a callback to the `transform` method. The contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merge in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provide a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be call if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.16":{"name":"csv","version":"0.0.16","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","_npmUser":{"name":"david","email":"david@adaltas.com"},"_id":"csv@0.0.16","_engineSupported":true,"_npmVersion":"1.1.21","_nodeVersion":"v0.6.18","_defaultsLoaded":true,"dist":{"shasum":"25d0cdd140801838afdb47c81645b008cf1b1f5b","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.16.tgz"},"readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provide CSV parsing and has been tested and used on large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Asynch and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath(data, options)*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *fromStream(readStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n    \n-   *from(data, options)*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, default to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, default to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns section below.\n\t\n-   *encoding*    \n    Default to 'utf8', apply when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, default to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), default to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), default to false.\n\nWriting API\n-----------\n\nThe following method are available:\n\n-   *write(data, preserve)*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end()*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath(path, options)*    \n    Take a file path as first argument and optionally on object of options as a second arguments.\n    \n-   *toStream(writeStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second arguments.\n\nOptions are:\n\n-   *delimiter*    \n    Default to the delimiter read option.\n    \n-   *quote*    \n    Default to the quote read option.\n    \n-   *escape*    \n    Default to the escape read option.\n    \n-   *columns*    \n    List of fields, apply when `transform` return an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Default to 'utf8', apply when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; default to 'auto' (discovered in source or 'unix' if no source is specified).\n    \n-   *flags*    \n    Default to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Apply when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flush into a stream. Apply when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\nTransforming data\n-----------------\n\n-   *transform(callback)*\n    User provided function call on each line to filter, enriche or modify the dataset. The callback is called asynchronously.\n\nThe contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merge in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provide a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be call if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.17":{"name":"csv","version":"0.0.17","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provides CSV parsing and has been tested and used on a large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Async and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath(data, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *fromStream(readStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n    \n-   *from(data, options)*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, defaults to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns sections below.\n\t\n-   *encoding*    \n    Defaults to 'utf8', applied when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, defaults to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), defaults to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), defaults to false.\n\nWriting API\n-----------\n\nThe following methods are available:\n\n-   *write(data, preserve)*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end()*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath(path, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *toStream(writeStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Defaults to the delimiter read option.\n    \n-   *quote*    \n    Defaults to the quote read option.\n    \n-   *quoted*    \n    Boolean, default to false, quote all the fields even if not required.\n    \n-   *escape*    \n    Defaults to the escape read option.\n    \n-   *columns*    \n    List of fields, applied when `transform` returns an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Defaults to 'utf8', applied when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; defaults to 'auto' (discovered in source or 'unix' if no source is specified).\n    \n-   *flags*    \n    Defaults to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Applied when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flushed into a stream. Applied when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\nTransforming data\n-----------------\n\n-   *transform(callback)*\n    User provided function call on each line to filter, enrich or modify the dataset. The callback is called asynchronously.\n\nThe contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merged in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provides a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be called if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","_id":"csv@0.0.17","dist":{"shasum":"4ad2d5f518bed952f56d043f281b9c85ebd344ea","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.17.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.18":{"name":"csv","version":"0.0.18","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"<pre>\n _   _           _        _____  _______      __ _____  \n| \\ | |         | |      / ____|/ ____\\ \\    / /|  __ \\  \n|  \\| | ___   __| | ___ | |    | (___  \\ \\  / / | |__) |_ _ _ __ ___  ___ _ __ \n| . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  |  ___/ _` | '__/ __|/ _ \\ '__|\n| |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   | |  | (_| | |  \\__ \\  __/ |  \n|_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/    |_|   \\__,_|_|  |___/\\___|_|  \n\n</pre>\n\nThis project provides CSV parsing and has been tested and used on a large source file (over 2Gb).\n\n-   Support delimiter, quote and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Async and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n\t// node samples/sample.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/sample.in')\n\t.toPath(__dirname+'/sample.out')\n\t.transform(function(data){\n\t\tdata.unshift(data.pop());\n\t\treturn data;\n\t})\n\t.on('data',function(data,index){\n\t\tconsole.log('#'+index+' '+JSON.stringify(data));\n\t})\n\t.on('end',function(count){\n\t\tconsole.log('Number of lines: '+count);\n\t})\n\t.on('error',function(error){\n\t\tconsole.log(error.message);\n\t});\n\t\n\t// Print sth like:\n\t// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n\t// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n\t// Number of lines: 2\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n    $ git clone http://github.com/wdavidw/node-csv-parser.git\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n    $ npm install csv\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath(data, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *fromStream(readStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n    \n-   *from(data, options)*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, defaults to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns sections below.\n\t\n-   *encoding*    \n    Defaults to 'utf8', applied when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, defaults to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), defaults to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), defaults to false.\n\nWriting API\n-----------\n\nThe following methods are available:\n\n-   *write(data, preserve)*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end()*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath(path, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *toStream(writeStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Defaults to the delimiter read option.\n    \n-   *quote*    \n    Defaults to the quote read option.\n    \n-   *quoted*    \n    Boolean, default to false, quote all the fields even if not required.\n    \n-   *escape*    \n    Defaults to the escape read option.\n    \n-   *columns*    \n    List of fields, applied when `transform` returns an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Defaults to 'utf8', applied when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; defaults to 'auto' (discovered in source or 'unix' if no source is specified).\n    \n-   *flags*    \n    Defaults to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Applied when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flushed into a stream. Applied when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\nTransforming data\n-----------------\n\n-   *transform(callback)*\n    User provided function call on each line to filter, enrich or modify the dataset. The callback is called asynchronously.\n\nThe contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merged in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n\t// node samples/transform.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/transform.in')\n\t.toStream(process.stdout)\n\t.transform(function(data,index){\n\t\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n\t});\n\t\n\t// Print sth like:\n\t// 82:Zbigniew Preisner,94:Serge Gainsbourg\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provides a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be called if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n\t// node samples/column.js\n\tvar csv = require('csv');\n\t\n\tcsv()\n\t.fromPath(__dirname+'/columns.in',{\n\t\tcolumns: true\n\t})\n\t.toStream(process.stdout,{\n\t\tcolumns: ['id', 'name']\n\t})\n\t.transform(function(data){\n\t\tdata.name = data.firstname + ' ' + data.lastname\n\t\treturn data;\n\t});\n\t\n\t// Print sth like:\n\t// 82,Zbigniew Preisner\n\t// 94,Serge Gainsbourg\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n\texpresso test\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","_id":"csv@0.0.18","dist":{"shasum":"7e322d2fdc640be7ef126d8cd1351333d1e1a632","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.18.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.0.19":{"name":"csv","version":"0.0.19","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used on a large source file (over 2Gb).\n\n-   Support delimiters, quotes and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Async and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n```javascript\n// node samples/sample.js\nvar csv = require('csv');\n\ncsv()\n.fromPath(__dirname+'/sample.in')\n.toPath(__dirname+'/sample.out')\n.transform(function(data){\n\tdata.unshift(data.pop());\n\treturn data;\n})\n.on('data',function(data,index){\n\tconsole.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end',function(count){\n\tconsole.log('Number of lines: '+count);\n})\n.on('error',function(error){\n\tconsole.log(error.message);\n});\n\n// Print sth like:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n```bash\ngit clone http://github.com/wdavidw/node-csv-parser.git\n```\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n```bash\nnpm install csv\n```\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath(data, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *fromStream(readStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n    \n-   *from(data, options)*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, defaults to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns sections below.\n\t\n-   *encoding*    \n    Defaults to 'utf8', applied when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, defaults to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), defaults to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), defaults to false.\n\nWriting API\n-----------\n\nThe following methods are available:\n\n-   *write(data, preserve)*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end()*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath(path, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *toStream(writeStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Defaults to the delimiter read option.\n    \n-   *quote*    \n    Defaults to the quote read option.\n    \n-   *quoted*    \n    Boolean, default to false, quote all the fields even if not required.\n    \n-   *escape*    \n    Defaults to the escape read option.\n    \n-   *columns*    \n    List of fields, applied when `transform` returns an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Defaults to 'utf8', applied when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; defaults to 'auto' (discovered in source or 'unix' if no source is specified).\n    \n-   *flags*    \n    Defaults to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Applied when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flushed into a stream. Applied when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\nTransforming data\n-----------------\n\n-   *transform(callback)*\n    User provided function call on each line to filter, enrich or modify the dataset. The callback is called asynchronously.\n\nThe contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merged in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n```javascript\n// node samples/transform.js\nvar csv = require('csv');\n\ncsv()\n.fromPath(__dirname+'/transform.in')\n.toStream(process.stdout)\n.transform(function(data,index){\n\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n});\n\n// Print sth like:\n// 82:Zbigniew Preisner,94:Serge Gainsbourg\n```\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provides a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be called if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n```javascript\n// node samples/column.js\nvar csv = require('csv');\n\ncsv()\n.fromPath(__dirname+'/columns.in',{\n\tcolumns: true\n})\n.toStream(process.stdout,{\n\tcolumns: ['id', 'name']\n})\n.transform(function(data){\n\tdata.name = data.firstname + ' ' + data.lastname\n\treturn data;\n});\n\n// Print sth like:\n// 82,Zbigniew Preisner\n// 94,Serge Gainsbourg\n```\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n```bash\nexpresso test\n```\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","_id":"csv@0.0.19","dist":{"shasum":"c90c1cad41433e9616d12bc5ca603ccd566b73d9","tarball":"http://registry.npmjs.org/csv/-/csv-0.0.19.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.0":{"name":"csv","version":"0.2.0","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","glob":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\n[Documentation is for the parser is available here](http://localhost:4000/projects/node-csv/).\n\nImportant\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser.\n\nThe documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nMigration\n---------\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now recieved a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nContributors\n------------\n\n*\t  David Worms : <https://github.com/wdavidw>\n*\t  Will White : <https://github.com/willwhite>\n*\t  Justin Latimer : <https://github.com/justinlatimer>\n*\t  jonseymour : <https://github.com/jonseymour>\n*\t  pascalopitz : <https://github.com/pascalopitz>\n*\t  Josh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","_id":"csv@0.2.0","dist":{"shasum":"aa4542c7677051447ffde7148d271b6820558543","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.0.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.1":{"name":"csv","version":"0.2.1","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","glob":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\n[Documentation is for the parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nImportant\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser.\n\nThe documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nQuick example\n-------------\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\nAdvanced example\n----------------\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(data){\n  data.unshift(data.pop());\n  return data;\n})\n.on('record', function(data,index){\n  console.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now recieved a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms : <https://github.com/wdavidw>\n*\t  Will White : <https://github.com/willwhite>\n*\t  Justin Latimer : <https://github.com/justinlatimer>\n*\t  jonseymour : <https://github.com/jonseymour>\n*\t  pascalopitz : <https://github.com/pascalopitz>\n*\t  Josh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","_id":"csv@0.2.1","dist":{"shasum":"ee18ba742e0fc89c5eb198c4e76f53d3ead9ebe2","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.1.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.2":{"name":"csv","version":"0.2.2","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","glob":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\n[Documentation is for the parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nImportant\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser.\n\nThe documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nQuick example\n-------------\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\nAdvanced example\n----------------\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(data){\n  data.unshift(data.pop());\n  return data;\n})\n.on('record', function(data,index){\n  console.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now recieved a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","_id":"csv@0.2.2","dist":{"shasum":"160eaf831345277cfd55d3a61b485f20de94cc6f","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.2.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.3":{"name":"csv","version":"0.2.3","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","glob":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nImportant\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser.\n\nThe documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nInstall command is `npm install csv`.\n\nQuick example\n-------------\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\nAdvanced example\n----------------\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(data){\n  data.unshift(data.pop());\n  return data;\n})\n.on('record', function(data,index){\n  console.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now recieved a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","_id":"csv@0.2.3","dist":{"shasum":"5eb70ef3b424e9d266470d004aaa74414283606f","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.3.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.1.0":{"name":"csv","version":"0.1.0","description":"CSV parser with simple api, full of options and tested against large datasets.","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"homepage":"https://github.com/wdavidw/node-csv-parser","readme":"<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used on a large source file (over 2Gb).\n\n-   Support delimiters, quotes and escape characters\n-   Line breaks discovery: line breaks in source are detected and reported to destination\n-   Data transformation\n-   Async and event based\n-   Support for large datasets\n-   Complete test coverage as sample and inspiration\n\nQuick example\n-------------\n\nUsing the library is a 4 steps process:\n\n1.\tCreate a source\n2.\tCreate a destination (optional)\n3.\tTransform the data (optional)\n4.  Listen to events (optional)\n\nHere is a example:\n\n```javascript\n// node samples/sample.js\nvar csv = require('csv');\n\ncsv()\n.fromPath(__dirname+'/sample.in')\n.toPath(__dirname+'/sample.out')\n.transform(function(data){\n\tdata.unshift(data.pop());\n\treturn data;\n})\n.on('data',function(data,index){\n\tconsole.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end',function(count){\n\tconsole.log('Number of lines: '+count);\n})\n.on('error',function(error){\n\tconsole.log(error.message);\n});\n\n// Print sth like:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nInstalling\n----------\n\nVia git (or downloaded tarball):\n\n```bash\ngit clone http://github.com/wdavidw/node-csv-parser.git\n```\n\nThen, simply copy or link the ./lib/csv.js file into your $HOME/.node_libraries folder or inside a declared path folder.\n\nVia [npm](http://github.com/isaacs/npm):\n\n```bash\nnpm install csv\n```\n\nReading API\n-----------\n\nThe following method are available:\n\n-   *fromPath(data, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *fromStream(readStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n    \n-   *from(data, options)*    \n    Take a string, a buffer, an array or an object as first argument and optionally some options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Set the field delimiter, one character only, defaults to comma.\n    \n-   *quote*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *escape*    \n    Set the field delimiter, one character only, defaults to double quotes.\n    \n-   *columns*    \n    List of fields or true if autodiscovered in the first CSV line, impact the `transform` argument and the `data` event by providing an object instead of an array, order matters, see the transform and the columns sections below.\n\t\n-   *encoding*    \n    Defaults to 'utf8', applied when a readable stream is created.\n\t\n-   *trim*    \n    If true, ignore whitespace immediately around the delimiter, defaults to false.\n\t\n-   *ltrim*    \n    If true, ignore whitespace immediately following the delimiter (i.e. left-trim all fields), defaults to false.\n\t\n-   *rtrim*    \n    If true, ignore whitespace immediately preceding the delimiter (i.e. right-trim all fields), defaults to false.\n\nWriting API\n-----------\n\nThe following methods are available:\n\n-   *write(data, preserve)*    \n    Take a string, an array or an object, implementation of the StreamWriter API.\n\t\n-   *end()*    \n    Terminate the stream, implementation of the StreamWriter API.\n    \n-   *toPath(path, options)*    \n    Take a file path as first argument and optionally on object of options as a second argument.\n    \n-   *toStream(writeStream, options)*    \n    Take a readable stream as first argument and optionally on object of options as a second argument.\n\nOptions are:\n\n-   *delimiter*    \n    Defaults to the delimiter read option.\n    \n-   *quote*    \n    Defaults to the quote read option.\n    \n-   *quoted*    \n    Boolean, default to false, quote all the fields even if not required.\n    \n-   *escape*    \n    Defaults to the escape read option.\n    \n-   *columns*    \n    List of fields, applied when `transform` returns an object, order matters, see the transform and the columns sections below.\n    \n-   *encoding*    \n    Defaults to 'utf8', applied when a writable stream is created.\n    \n-   *header*\n    Display the column names on the first line if the columns option is provided.\n\n-   *lineBreaks*    \n    String used to delimit record rows or a special value; special values are 'auto', 'unix', 'mac', 'windows', 'unicode'; defaults to 'auto' (discovered in source or 'unix' if no source is specified).\n    \n-   *flags*    \n    Defaults to 'w', 'w' to create or overwrite an file, 'a' to append to a file. Applied when using the `toPath` method.\n    \n-   *bufferSize*    \n    Internal buffer holding data before being flushed into a stream. Applied when destination is a stream.\n    \n-   *end*    \n    Prevent calling `end` on the destination, so that destination is no longer writable, similar to passing `{end: false}` option in `stream.pipe()`.\n\n-   *newColumns*\n    If the `columns` option is not specified (which means columns will be taken from the reader\n    options, will automatically append new columns if they are added during `transform()`.\n\nTransforming data\n-----------------\n\n-   *transform(callback)*\n    User provided function call on each line to filter, enrich or modify the dataset. The callback is called asynchronously.\n\nThe contract is quite simple, you receive an array of fields for each record and return the transformed record. The return value may be an array, an associative array, a string or null. If null, the record will simply be skipped.\n\nUnless you specify the `columns` read option, `data` are provided as arrays, otherwise they are objects with keys matching columns names.\n\nWhen the returned value is an array, the fields are merged in order. When the returned value is an object, it will search for the `columns` property in the write or in the read options and smartly order the values. If no `columns` options are found, it will merge the values in their order of appearance. When the returned value is a string, it is directly sent to the destination source and it is your responsibility to delimit, quote, escape or define line breaks.\n\nExample of transform returning a string\n\n```javascript\n// node samples/transform.js\nvar csv = require('csv');\n\ncsv()\n.fromPath(__dirname+'/transform.in')\n.toStream(process.stdout)\n.transform(function(data,index){\n\treturn (index>0 ? ',' : '') + data[0] + \":\" + data[2] + ' ' + data[1];\n});\n\n// Print sth like:\n// 82:Zbigniew Preisner,94:Serge Gainsbourg\n```\n\nEvents\n------\n\nBy extending the Node `EventEmitter` class, the library provides a few useful events:\n\n-\t*data* (function(data, index){})\n    Thrown when a new row is parsed after the `transform` callback and with the data being the value returned by `transform`. Note however that the event won't be called if transform return `null` since the record is skipped.\n\tThe callback provide two arguments:\n\t`data` is the CSV line being processed (by default as an array)\n\t`index` is the index number of the line starting at zero\n    \n-   *end*\n    In case your redirecting the output to a file using the `toPath` method, the event will be called once the writing process is complete and the file closed.\n    \n-   *error*\n    Thrown whenever an error is captured.\n\nColumns\n-------\n\nColumns names may be provided or discovered in the first line with the read options `columns`. If defined as an array, the order must match the one of the input source. If set to `true`, the fields are expected to be present in the first line of the input source.\n\nYou can define a different order and even different columns in the read options and in the write options. If the `columns` is not defined in the write options, it will default to the one present in the read options. \n\nWhen working with fields, the `transform` method and the `data` events receive their `data` parameter as an object instead of an array where the keys are the field names.\n\n```javascript\n// node samples/column.js\nvar csv = require('csv');\n\ncsv()\n.fromPath(__dirname+'/columns.in',{\n\tcolumns: true\n})\n.toStream(process.stdout,{\n\tcolumns: ['id', 'name']\n})\n.transform(function(data){\n\tdata.name = data.firstname + ' ' + data.lastname\n\treturn data;\n});\n\n// Print sth like:\n// 82,Zbigniew Preisner\n// 94,Serge Gainsbourg\n```\n\nRunning the tests\n-----------------\n\nTests are executed with expresso. To install it, simple use `npm install -g expresso`.\n\nTo run the tests\n```bash\nexpresso test\n```\n\nContributors\n------------\n\n*\tDavid Worms : <https://github.com/wdavidw>\n*\tWill White : <https://github.com/willwhite>\n*\tJustin Latimer : <https://github.com/justinlatimer>\n*\tjonseymour : <https://github.com/jonseymour>\n*\tpascalopitz : <https://github.com/pascalopitz>\n*\tJosh Pschorr : <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n","_id":"csv@0.1.0","dist":{"shasum":"139a0141e91494040b8727358af899aefa6d27e0","tarball":"http://registry.npmjs.org/csv/-/csv-0.1.0.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.4":{"name":"csv","version":"0.2.4","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","glob":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nImportant\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser.\n\nThe documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nInstall command is `npm install csv`.\n\nQuick example\n-------------\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\nAdvanced example\n----------------\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(data){\n  data.unshift(data.pop());\n  return data;\n})\n.on('record', function(data,index){\n  console.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now recieved a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","_id":"csv@0.2.4","dist":{"shasum":"142ab5abdc4987e9be55d66452dd548728f07c6d","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.4.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.6":{"name":"csv","version":"0.2.6","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","glob":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nImportant\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser.\n\nThe documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nInstall command is `npm install csv`.\n\nQuick example\n-------------\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\nAdvanced example\n----------------\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(data){\n  data.unshift(data.pop());\n  return data;\n})\n.on('record', function(data,index){\n  console.log('#'+index+' '+JSON.stringify(data));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now recieved a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","_id":"csv@0.2.6","dist":{"shasum":"7811d839bd328b33539698c27094c28b2a4d2a52","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.6.tgz"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.7":{"name":"csv","version":"0.2.7","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","bugs":"https://github.com/wdavidw/node-csv-parser/issues","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used \non large input files. It provide every option you could expect from an\nadvanced CSV parser and stringifier.\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nUsage\n-----\n\nInstallation command is `npm install csv`.\n\n### Quick example\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\n### Advanced example\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(row){\n  row.unshift(row.pop());\n  return row;\n})\n.on('record', function(row,index){\n  console.log('#'+index+' '+JSON.stringify(row));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser. The documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now receives a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","readmeFilename":"README.md","_id":"csv@0.2.7","dist":{"shasum":"784a69370c8951f6fc38840535015525ae82c80f","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.7.tgz"},"_from":".","_npmVersion":"1.2.3","_npmUser":{"name":"david","email":"david@adaltas.com"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.8":{"name":"csv","version":"0.2.8","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","bugs":"https://github.com/wdavidw/node-csv-parser/issues","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","iconv":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used \non large input files. It provide every option you could expect from an\nadvanced CSV parser and stringifier.\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nUsage\n-----\n\nInstallation command is `npm install csv`.\n\n### Quick example\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\n### Advanced example\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(row){\n  row.unshift(row.pop());\n  return row;\n})\n.on('record', function(row,index){\n  console.log('#'+index+' '+JSON.stringify(row));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser. The documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now receives a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","readmeFilename":"README.md","_id":"csv@0.2.8","dist":{"shasum":"53b2e19f62a6f8b0ca2d9e91024c65cc41887fb1","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.8.tgz"},"_from":".","_npmVersion":"1.2.11","_npmUser":{"name":"david","email":"david@adaltas.com"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.2.9":{"name":"csv","version":"0.2.9","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","bugs":"https://github.com/wdavidw/node-csv-parser/issues","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","iconv":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used \non large input files. It provide every option you could expect from an\nadvanced CSV parser and stringifier.\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nUsage\n-----\n\nInstallation command is `npm install csv`.\n\n### Quick example\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\n### Advanced example\n    \n```javascript\n// node samples/sample.js\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(row){\n  row.unshift(row.pop());\n  return row;\n})\n.on('record', function(row,index){\n  console.log('#'+index+' '+JSON.stringify(row));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser. The documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now receives a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","readmeFilename":"README.md","_id":"csv@0.2.9","dist":{"shasum":"52d16e7ec2d61971211bf326f49a8294efb65f16","tarball":"http://registry.npmjs.org/csv/-/csv-0.2.9.tgz"},"_from":".","_npmVersion":"1.2.11","_npmUser":{"name":"david","email":"david@adaltas.com"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.3.0":{"name":"csv","version":"0.3.0","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","bugs":"https://github.com/wdavidw/node-csv-parser/issues","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv-parser.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","iconv":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used \non large input files. It provide every option you could expect from an\nadvanced CSV parser and stringifier.\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nUsage\n-----\n\nInstallation command is `npm install csv`.\n\n### Quick example\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\n### Advanced example\n    \n```javascript\n// node samples/sample.js\nvar fs = require('fs');\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(row){\n  row.unshift(row.pop());\n  return row;\n})\n.on('record', function(row,index){\n  console.log('#'+index+' '+JSON.stringify(row));\n})\n.on('end', function(count){\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThis readme cover the current version 0.2.x of the node \ncsv parser. The documentation for the current version 0.1.0 is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now receives a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","readmeFilename":"README.md","_id":"csv@0.3.0","dist":{"shasum":"0f138402e01b16de2c7d546c23d65eac9db60b2c","tarball":"http://registry.npmjs.org/csv/-/csv-0.3.0.tgz"},"_from":".","_npmVersion":"1.2.11","_npmUser":{"name":"david","email":"david@adaltas.com"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.3.2":{"name":"csv","version":"0.3.2","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","bugs":"https://github.com/wdavidw/node-csv/issues","author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","iconv":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used \non large input files. It provide every option you could expect from an\nadvanced CSV parser and stringifier.\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nUsage\n-----\n\nInstallation command is `npm install csv`.\n\n### Quick example\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\n### Advanced example\n    \n```javascript\n// node samples/sample.js\nvar fs = require('fs');\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(row){\n  row.unshift(row.pop());\n  return row;\n})\n.on('record', function(row,index){\n  console.log('#'+index+' '+JSON.stringify(row));\n})\n.on('close', function(count){\n  // when writing to a file, use the 'close' event\n  // the 'end' event may fire before the file has been written\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThis README covers the current version 0.2.x of the `node \ncsv `parser. The documentation for the previous version (0.1.0) is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now receives a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","readmeFilename":"README.md","_id":"csv@0.3.2","dist":{"shasum":"948e0c39db9d94935efcdfe36bb8aef1e8b10a7c","tarball":"http://registry.npmjs.org/csv/-/csv-0.3.2.tgz"},"_from":".","_npmVersion":"1.2.18","_npmUser":{"name":"david","email":"david@adaltas.com"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}},"0.3.3":{"name":"csv","version":"0.3.3","description":"CSV parser with simple api, full of options and tested against large datasets.","homepage":"http://www.adaltas.com/projects/node-csv/","bugs":{"url":"https://github.com/wdavidw/node-csv/issues"},"author":{"name":"David Worms","email":"david@adaltas.com"},"contributors":[{"name":"David Worms","email":"david@adaltas.com"},{"name":"Will White","email":"https://github.com/willwhite"},{"name":"Justin Latimer","email":"https://github.com/justinlatimer"},{"name":"jonseymour","email":"https://github.com/jonseymour"},{"name":"pascalopitz","email":"https://github.com/pascalopitz"},{"name":"Josh Pschorr","email":"https://github.com/jpschorr"},{"name":"Elad Ben-Israel","email":"https://github.com/eladb"},{"name":"Philippe Plantier","email":"https://github.com/phipla"},{"name":"Tim Oxley","email":"https://github.com/timoxley"},{"name":"Damon Oehlman","email":"https://github.com/DamonOehlman"},{"name":"Alexandru Topliceanu","email":"https://github.com/topliceanu"},{"name":"Visup","email":"https://github.com/visup"},{"name":"Edmund von der Burg","email":"https://github.com/evdb"},{"name":"Douglas Christopher Wilson","email":"https://github.com/dougwilson"}],"engines":{"node":">= 0.1.90"},"keywords":["node","parser","csv"],"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv.git"},"devDependencies":{"coffee-script":"latest","mocha":"latest","should":"latest","each":"latest","mecano":"latest","iconv":"latest"},"dependencies":{},"optionalDependencies":{},"scripts":{"test":"make test"},"readme":"[![Build Status](https://secure.travis-ci.org/wdavidw/node-csv-parser.png)](http://travis-ci.org/wdavidw/node-csv-parser)\n\n<pre>\n     _   _           _        _____  _______      __\n    | \\ | |         | |      / ____|/ ____\\ \\    / /\n    |  \\| | ___   __| | ___ | |    | (___  \\ \\  / / \n    | . ` |/ _ \\ / _` |/ _ \\| |     \\___ \\  \\ \\/ /  \n    | |\\  | (_) | (_| |  __/| |____ ____) |  \\  /   \n    |_| \\_|\\___/ \\__,_|\\___| \\_____|_____/    \\/     New BSD License\n\n</pre>\n\nThis project provides CSV parsing and has been tested and used \non large input files. It provide every option you could expect from an\nadvanced CSV parser and stringifier.\n\n[Documentation for the CSV parser is available here](http://www.adaltas.com/projects/node-csv/).\n\nUsage\n-----\n\nInstallation command is `npm install csv`.\n\n### Quick example\n\n```javascript\n// node samples/string.js\nvar csv = require('csv');\ncsv()\n.from( '\"1\",\"2\",\"3\",\"4\"\\n\"a\",\"b\",\"c\",\"d\"' )\n.to( console.log )\n// Output:\n// 1,2,3,4\n// a,b,c,d\n```\n\n### Advanced example\n    \n```javascript\n// node samples/sample.js\nvar fs = require('fs');\nvar csv = require('csv');\ncsv()\n.from.stream(fs.createReadStream(__dirname+'/sample.in'))\n.to.path(__dirname+'/sample.out')\n.transform( function(row){\n  row.unshift(row.pop());\n  return row;\n})\n.on('record', function(row,index){\n  console.log('#'+index+' '+JSON.stringify(row));\n})\n.on('close', function(count){\n  // when writing to a file, use the 'close' event\n  // the 'end' event may fire before the file has been written\n  console.log('Number of lines: '+count);\n})\n.on('error', function(error){\n  console.log(error.message);\n});\n// Output:\n// #0 [\"2000-01-01\",\"20322051544\",\"1979.0\",\"8.8017226E7\",\"ABC\",\"45\"]\n// #1 [\"2050-11-27\",\"28392898392\",\"1974.0\",\"8.8392926E7\",\"DEF\",\"23\"]\n// Number of lines: 2\n```\n\nMigration\n---------\n\nThis README covers the current version 0.2.x of the `node \ncsv `parser. The documentation for the previous version (0.1.0) is \navailable [here](https://github.com/wdavidw/node-csv-parser/tree/v0.1).\n\nThe functions 'from*' and 'to*' are now rewritten as 'from.*' and 'to.*'. The 'data'\nevent is now the 'record' event. The 'data' now receives a stringified version of \nthe 'record' event.\n\nDevelopment\n-----------\n\nTests are executed with mocha. To install it, simple run `npm install`, it will install\nmocha and its dependencies in your project \"node_modules\" directory.\n\nTo run the tests:\n```bash\nnpm test\n```\n\nThe tests run against the CoffeeScript source files.\n\nTo generate the JavaScript files:\n```bash\nmake build\n```\n\nThe test suite is run online with [Travis][travis] against Node.js version 0.6, 0.7, 0.8 and 0.9.\n\nContributors\n------------\n\n*\t  David Worms: <https://github.com/wdavidw>\n*\t  Will White: <https://github.com/willwhite>\n*\t  Justin Latimer: <https://github.com/justinlatimer>\n*\t  jonseymour: <https://github.com/jonseymour>\n*\t  pascalopitz: <https://github.com/pascalopitz>\n*\t  Josh Pschorr: <https://github.com/jpschorr>\n*   Elad Ben-Israel: <https://github.com/eladb>\n*   Philippe Plantier: <https://github.com/phipla>\n*   Tim Oxley: <https://github.com/timoxley>\n*   Damon Oehlman: <https://github.com/DamonOehlman>\n*   Alexandru Topliceanu: <https://github.com/topliceanu>\n*   Visup: <https://github.com/visup>\n*   Edmund von der Burg: <https://github.com/evdb>\n*   Douglas Christopher Wilson: <https://github.com/dougwilson>\n*   Chris Khoo: <https://github.com/khoomeister>\n\nRelated projects\n----------------\n\n*   Pavel Kolesnikov \"ya-csv\": <http://github.com/koles/ya-csv>\n*   Chris Williams \"node-csv\": <http://github.com/voodootikigod/node-csv>\n\n[travis]: https://travis-ci.org/#!/wdavidw/node-csv-parser\n\n","readmeFilename":"README.md","_id":"csv@0.3.3","dist":{"shasum":"a2c84cad2209123ef7b2070df31d192f3cdae71b","tarball":"http://registry.npmjs.org/csv/-/csv-0.3.3.tgz"},"_from":".","_npmVersion":"1.2.24","_npmUser":{"name":"david","email":"david@adaltas.com"},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"directories":{}}},"maintainers":[{"name":"david","email":"david@adaltas.com"}],"author":{"name":"David Worms","email":"david@adaltas.com"},"repository":{"type":"git","url":"git://github.com/wdavidw/node-csv.git"},"time":{"0.0.1":"2011-01-21T11:48:11.161Z","0.0.2":"2011-01-21T11:48:11.161Z","0.0.3":"2011-01-21T11:48:11.161Z","0.0.5":"2011-01-21T11:48:11.161Z","0.0.6":"2011-01-27T15:08:41.875Z","0.0.7":"2011-02-14T21:26:47.782Z","0.0.9":"2011-04-10T14:57:53.983Z","0.0.10":"2011-09-05T22:28:30.155Z","0.0.11":"2012-02-12T22:21:31.636Z","0.0.12":"2012-04-01T14:58:51.441Z","0.0.13":"2012-04-03T21:44:28.157Z","0.0.14":"2012-06-20T13:37:44.683Z","0.0.15":"2012-06-20T15:11:03.250Z","0.0.16":"2012-07-01T10:51:37.680Z","0.0.17":"2012-07-20T21:05:49.092Z","0.0.18":"2012-07-26T13:31:27.842Z","0.0.19":"2012-08-15T21:16:30.594Z","0.2.0":"2012-10-02T15:48:14.458Z","0.2.1":"2012-10-24T13:11:28.769Z","0.2.2":"2012-11-05T21:11:36.587Z","0.2.3":"2012-11-16T16:49:00.509Z","0.1.0":"2012-11-19T16:00:28.130Z","0.2.4":"2012-11-20T11:52:09.317Z","0.2.6":"2013-01-02T12:00:04.326Z","0.2.7":"2013-02-09T22:29:34.245Z","0.2.8":"2013-03-13T09:19:58.205Z","0.2.9":"2013-03-17T09:06:38.030Z","0.3.0":"2013-04-10T21:59:05.667Z","0.3.2":"2013-05-29T21:51:17.006Z","0.3.3":"2013-06-13T06:45:29.787Z"},"users":{"fgribreau":true,"david":true,"konklone":true,"simonvwade":true},"_attachments":{"csv-0.3.3.tgz":{"content_type":"application/octet-stream","revpos":95,"digest":"md5-qawYkPAmEIbTpic124qN6A==","length":76320,"stub":true},"csv-0.3.2.tgz":{"content_type":"application/octet-stream","revpos":93,"digest":"md5-bmLYr7GgHmQoGD8rxUlEFg==","length":76193,"stub":true},"csv-0.3.0.tgz":{"content_type":"application/octet-stream","revpos":87,"digest":"md5-kkmIFuW+xddo4mKXztveTQ==","length":73587,"stub":true},"csv-0.2.9.tgz":{"content_type":"application/octet-stream","revpos":85,"digest":"md5-RFRyaaZf2I7S1gWL5NVJ1Q==","length":72851,"stub":true},"csv-0.2.8.tgz":{"content_type":"application/octet-stream","revpos":83,"digest":"md5-ojqix1gwXz8HvIJ9b9heJQ==","length":72707,"stub":true},"csv-0.2.7.tgz":{"content_type":"application/octet-stream","revpos":80,"digest":"md5-KU490VJ3p1XXbPKEDMTvZg==","length":71304,"stub":true},"csv-0.2.6.tgz":{"content_type":"application/octet-stream","revpos":78,"digest":"md5-c6iOju/y1HEaUDfDvpQFCQ==","length":78764,"stub":true},"csv-0.2.4.tgz":{"content_type":"application/octet-stream","revpos":75,"digest":"md5-JNN4Imy06O+ZHSKurM0Z0w==","length":77892,"stub":true},"csv-0.1.0.tgz":{"content_type":"application/octet-stream","revpos":71,"digest":"md5-buKlWLOOSdBXVNqnHQHl3w==","length":23076,"stub":true},"csv-0.2.3.tgz":{"content_type":"application/octet-stream","revpos":50,"digest":"md5-ofQKnnOakRGXQOZFYGqVPw==","length":77624,"stub":true},"csv-0.2.2.tgz":{"content_type":"application/octet-stream","revpos":47,"digest":"md5-h/A97wNrtm76dRCVe+3M4w==","length":70022,"stub":true},"csv-0.2.1.tgz":{"content_type":"application/octet-stream","revpos":45,"digest":"md5-r8MXoVFJseMeVzT2XrtDVg==","length":70605,"stub":true},"csv-0.2.0.tgz":{"content_type":"application/octet-stream","revpos":43,"digest":"md5-D+VMfqUU/3xXPoYcJePqCw==","length":61299,"stub":true},"csv-0.0.19.tgz":{"content_type":"application/octet-stream","revpos":41,"digest":"md5-xpp9JGlnr+IY+qN3lOJIvA==","length":23077,"stub":true},"csv-0.0.18.tgz":{"content_type":"application/octet-stream","revpos":37,"digest":"md5-TxcnUPiPlEtEqv+/KIJSOA==","length":22955,"stub":true},"csv-0.0.17.tgz":{"content_type":"application/octet-stream","revpos":35,"digest":"md5-RkiUzSMH3WrNEkBaFd6F4A==","length":22963,"stub":true},"csv-0.0.16.tgz":{"content_type":"application/octet-stream","revpos":33,"digest":"md5-egT9yPWXoGGnxfDqseg3kA==","length":22634,"stub":true},"csv-0.0.15.tgz":{"content_type":"application/octet-stream","revpos":31,"digest":"md5-b1eNBqb1FqGqbYKNaxaDEg==","length":22709,"stub":true},"csv-0.0.14.tgz":{"content_type":"application/octet-stream","revpos":29,"digest":"md5-Rt8sJnm9RDK9ug7VWDZoqA==","length":22835,"stub":true},"csv-0.0.13.tgz":{"content_type":"application/octet-stream","revpos":27,"digest":"md5-L6IoEGGAlhMqlsJLn6Y98g==","length":22225,"stub":true},"csv-0.0.12.tgz":{"content_type":"application/octet-stream","revpos":25,"digest":"md5-K1NDfWe9x44jidiKZQWBlg==","length":21947,"stub":true},"csv-0.0.11.tgz":{"content_type":"application/octet-stream","revpos":23,"digest":"md5-F5fLnrw6XHtgenb5JeBxkA==","length":21195,"stub":true},"csv-0.0.10.tgz":{"content_type":"application/octet-stream","revpos":21,"digest":"md5-yRoQXNn2Sws/gJqyP/mxIg==","length":23230,"stub":true},"csv-0.0.9.tgz":{"content_type":"application/octet-stream","revpos":19,"digest":"md5-0Vd52lBi6pGF14/XaxGzRA==","length":23879,"stub":true},"csv-0.0.7.tgz":{"content_type":"application/octet-stream","revpos":17,"digest":"md5-2hEP8ecMKuULAd4LKcCPLw==","length":18103,"stub":true},"csv-0.0.6.tgz":{"content_type":"application/octet-stream","revpos":15,"digest":"md5-g4+6WY5MJmokgS3RAhYgFw==","length":19227,"stub":true},"csv-0.0.5.tgz":{"content_type":"application/octet-stream","revpos":13,"digest":"md5-NXSNC6Tw9i0PVG3MEFYbuA==","length":18664,"stub":true},"csv-0.0.3.tgz":{"content_type":"application/octet-stream","revpos":11,"digest":"md5-UOOE29Df+phit97E4P2FXA==","length":17082,"stub":true},"csv-0.0.2.tgz":{"content_type":"application/octet-stream","revpos":5,"digest":"md5-OEtJcMEgLVHFvbTijfGBPw==","length":17256,"stub":true},"csv-0.0.1.tgz":{"content_type":"application/octet-stream","revpos":3,"digest":"md5-xmIP76cPQ9paLlV9PksgXQ==","length":15735,"stub":true}},"_etag":"\"36AOVWSRWFVVOL9CUM65BFOL4\""}